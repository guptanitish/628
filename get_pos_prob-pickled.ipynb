{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import math\n",
      "import copy\n",
      "import numpy as np\n",
      "from __future__ import division\n",
      "import pickle\n",
      "path = \"/home/stufs1/nitigupta/628/data/train_v2_tagged_full.txt\" #path to the tagged file\n",
      "test_data_path = \"/home/stufs1/nitigupta/628/sample_text_tagged.txt\"\n",
      "bi_word_path = \"/home/stufs1/nitigupta/628/pickled_ds/bi_word.txt\" #bigram word to word dict\n",
      "bi_arr_path = \"/home/stufs1/nitigupta/628/pickled_ds/bi_arr.txt\" #bigram pos to pos arr\n",
      "tri_arr_path = \"/home/stufs1/nitigupta/628/pickled_ds/tri_arr.txt\" #trigram pos to pos arr\n",
      "pos_prob_path = \"/home/stufs1/nitigupta/628/pickled_ds/pos_prob_path.txt\" #pos prob dict\n",
      "word_pos_path = \"/home/stufs1/nitigupta/628/pickled_ds/word_pos_path.txt\" #prev word to cur pos prob dict\n",
      "pos_word_dict_path = \"/home/stufs1/nitigupta/628/pickled_ds/pos_word_dict.txt\" #wprd to pos prob dict\n",
      "word_curpos_path = \"/home/stufs1/nitigupta/628/pickled_ds/pos_curword_dict.txt\" #cur wprd to cur pos prob dict\n",
      "output_path = \"/home/stufs1/nitigupta/628/pickled_ds/sents_with_predict_tags.txt\" #cur wprd to cur pos prob dict\n",
      "df = pd.DataFrame.from_csv(path,header=None,index_col=None,sep='\\t')\n",
      "test_data_df = pd.DataFrame.from_csv(test_data_path,header=None,index_col=None,sep='\\t')\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "L = len(df)\n",
      "def make_index(l,d):\n",
      "    for i in range (0,len(l)):\n",
      "        d[l[i]]=i\n",
      "        \n",
      "def save_to_disk(ds,path):\n",
      "    print(\"Pickling to... \",path)\n",
      "    dumpfile=open(path,'wb')\n",
      "    pickle.dump(ds,dumpfile)\n",
      "    dumpfile.close()\n",
      "    print(\"Done Pickling...\")\n",
      "    \n",
      "def load_from_disk(path):\n",
      "    print(\"Unpickling...\")\n",
      "    unpickled_file=open(path,'rb')\n",
      "    ds=pickle.load(unpickled_file)\n",
      "    print(\"Done Pickling...\")\n",
      "    return ds\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pos_tags_dict = dict()\n",
      "for i in range (0,L):\n",
      "    #print(df[][])\n",
      "    pos_tags_dict[df[1][i]]=1\n",
      "sorted(pos_tags_dict, key=pos_tags_dict.get)\n",
      "\n",
      "pos_tags_l = pos_tags_dict.keys()\n",
      "del pos_tags_dict\n",
      "pos_tags_l.sort()\n",
      "\n",
      "pos_tags_index_mapper = dict()\n",
      "make_index(pos_tags_l,pos_tags_index_mapper)\n",
      "del pos_tags_l\n",
      "\n",
      "pos_tags_count = len(pos_tags_index_mapper)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "word_dict = dict()\n",
      "for i in range (0,L):\n",
      "    #print(df[][])\n",
      "    word_dict[df[0][i]]=1\n",
      "sorted(word_dict, key=word_dict.get)\n",
      "\n",
      "word_l = word_dict.keys()\n",
      "del word_dict\n",
      "word_l.sort()\n",
      "\n",
      "word_index_mapper = dict()\n",
      "make_index(word_l,word_index_mapper)\n",
      "del word_l\n",
      "word_count = len(word_index_mapper)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def set_prob(arr,count1,count2):\n",
      "    \n",
      "    column_sum = np.zeros(count2)\n",
      "    for i in range(0,count1):\n",
      "        for j in range(0,count2):\n",
      "            column_sum[j]+=arr[i][j]\n",
      "    \n",
      "    for i in range(0,count1):\n",
      "        for j in range(0,count2):\n",
      "            arr[i][j] = arr[i][j]/column_sum[j]\n",
      "            "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "bi_arr = np.zeros( ( pos_tags_count,pos_tags_count) )\n",
      "\n",
      "for i in range(0,(L-1)):\n",
      "    #print(i)\n",
      "    bi_arr[pos_tags_index_mapper[df[1][i]]][pos_tags_index_mapper[df[1][i+1]]]+=1\n",
      "\n",
      "set_prob(bi_arr,len(bi_arr),len(bi_arr))\n",
      "total=0\n",
      "for i in range(0,len(bi_arr)):\n",
      "    total+=bi_arr[i][20]\n",
      "print(total)\n",
      "\n",
      "\n",
      "save_to_disk(bi_arr,bi_arr_path)\n",
      "#d = load_from_disk(bi_arr_path)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "1.0\n",
        "('Pickling to... ', '/home/stufs1/nitigupta/628/pickled_ds/bi_arr.txt')\n",
        "Done Pickling..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 29
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#import seaborn as sns\n",
      "bi_word_dict = dict()\n",
      "\n",
      "def update_dict(d):\n",
      "    \n",
      "    if w1 not in d:\n",
      "        d[w1]= {}\n",
      "        d[w1][w2]=1\n",
      "    else:\n",
      "        if w2 not in d[w1]:\n",
      "            d[w1][w2]=1\n",
      "        else:\n",
      "            d[w1][w2]+=1\n",
      "def convert_to_prob(d):\n",
      "    for outer_key in d:\n",
      "        count = 0\n",
      "        for inner_key in d[outer_key]:\n",
      "            count+=d[outer_key][inner_key]\n",
      "        for inner_key in d[outer_key]:\n",
      "            d[outer_key][inner_key] = d[outer_key][inner_key]/count\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bi_word_dict = dict()   \n",
      "\n",
      "for i in range(0,(L-1)):\n",
      "    \n",
      "    w2=df[0][i]\n",
      "    w1=df[0][i+1]\n",
      "    if type(w1)==str:\n",
      "        w1=w1.lower()\n",
      "    if type(w2)==str:\n",
      "        w2=w2.lower()\n",
      "    update_dict(bi_word_dict)\n",
      "\n",
      "\n",
      "#if probabilities are required, execute this also:\n",
      "convert_to_prob(bi_word_dict)\n",
      "\n",
      "save_to_disk(bi_word_dict,bi_word_path)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "pos_word_dict= dict()\n",
      "for i in range(0,L):   \n",
      "    w1=df[1][i] #pos\n",
      "    w2=df[0][i]\n",
      "    if type(w2)==str:\n",
      "        w2=w2.lower()\n",
      "    update_dict(pos_word_dict)\n",
      "\n",
      "save_to_disk(pos_word_dict,pos_word_dict_path)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "('Pickling to... ', '/home/stufs1/nitigupta/628/pickled_ds/pos_word_dict.txt')\n",
        "Done Pickling..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "pos_tags_prob = np.zeros(pos_tags_count)\n",
      "\n",
      "\n",
      "for i in range(0,L):\n",
      "    pos_tags_prob[pos_tags_index_mapper[df[1][i]]]+=1\n",
      "\n",
      "\n",
      "pos_tags_prob=pos_tags_prob/L\n",
      "\n",
      "#save_to_disk(pos_tags_prob,pos_prob_path)\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "word_arr = np.zeros( (word_count,pos_tags_count) )\n",
      "\n",
      "for i in range(0,(L-1)):\n",
      "    #print(i)\n",
      "    word_arr[word_index_mapper[df[0][i]]][pos_tags_index_mapper[df[1][i+1]]]+=1 \n",
      "    \n",
      "set_prob(word_arr,word_count,pos_tags_count)\n",
      "\n",
      "total=0\n",
      "for i in range(0,pos_tags_count):\n",
      "    total+=word_arr[20][i]\n",
      "print(total)\n",
      "\n",
      "#save_to_disk(word_arr,word_pos_path)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2.77955360369e-05\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "word_tag_arr = np.zeros( (word_count,pos_tags_count) )\n",
      "\n",
      "for i in range(0,(L-1)):\n",
      "    #print(i)\n",
      "    word_tag_arr[word_index_mapper[df[0][i]]][pos_tags_index_mapper[df[1][i]]]+=1 \n",
      "    \n",
      "set_prob(word_tag_arr,word_count,pos_tags_count)\n",
      "\n",
      "total=0\n",
      "for i in range(0,pos_tags_count):\n",
      "    total+=word_tag_arr[10][i]\n",
      "print(total)\n",
      "\n",
      "#save_to_disk(word_tag_arr,word_curpos_path)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.0153209109731\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tri_arr = np.zeros( (pos_tags_count,pos_tags_count,pos_tags_count) )\n",
      "\n",
      "for i in range(0,(L-2)):\n",
      "    #print(i)\n",
      "    tri_arr[pos_tags_index_mapper[df[1][i]]][pos_tags_index_mapper[df[1][i+1]]][pos_tags_index_mapper[df[1][i+2]]]+=1\n",
      "\n",
      "tri_column_sum=np.zeros(pos_tags_count)\n",
      "\n",
      "#column_sum = np.zeros(len(tri_arr))\n",
      "for k in range(0,len(tri_arr)):\n",
      "    for i in range(0,len(tri_arr)):\n",
      "        for j in range(0,len(tri_arr)):        \n",
      "            tri_column_sum[k]+=tri_arr[i][j][k]\n",
      "\n",
      "for k in range(0,len(tri_arr)):\n",
      "    for i in range(0,len(tri_arr)):\n",
      "        for j in range(0,len(tri_arr)):\n",
      "            tri_arr[i][j][k] = tri_arr[i][j][k]/(tri_column_sum[k]+1)\n",
      "\n",
      "total=0.0\n",
      "for i in range(0,len(tri_arr)):\n",
      "    for j in range(0,len(tri_arr)):\n",
      "        total+=tri_arr[i][j][20]\n",
      "    \n",
      "print(total)\n",
      "print(tri_arr[20][20][20])\n",
      "\n",
      "save_to_disk(tri_arr,tri_arr_path)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.999972205237\n",
        "0.00986714103063\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bi_arr = load_from_disk(bi_arr_path)\n",
      "tri_arr = load_from_disk(tri_arr_path)\n",
      "word_arr = load_from_disk(word_pos_path)\n",
      "\n",
      "L_test_data = len(test_data_df)\n",
      "counter = 1\n",
      "output = []\n",
      "sent = []\n",
      "sents = []\n",
      "for i in range(L_test_data):\n",
      "        if (test_data_df[0][i] == \". .\"):\n",
      "            sents.append(copy.deepcopy(sent))\n",
      "            #print sents\n",
      "            del sent[:]\n",
      "            continue\n",
      "        (word, tag) = test_data_df[0][i].split()\n",
      "        sent.append((word,tag))\n",
      "        \n",
      "for line in sents:\n",
      "    #print line\n",
      "    local_min = 999999\n",
      "    local_pos = 0\n",
      "    # Get the probabs and identify the missing location\n",
      "    for i in range(1,len(line)):\n",
      "        if (i == 1):\n",
      "            (prev_word, prev_tag) = line[i-1]\n",
      "            (cur_word, cur_tag) = line[i]\n",
      "            prob_prevtag_given_curtag = bi_arr[pos_tags_index_mapper[cur_tag],pos_tags_index_mapper[prev_tag]]*1000\n",
      "            prob_curtag_given_prevtag = bi_arr[pos_tags_index_mapper[prev_tag],pos_tags_index_mapper[cur_tag]]*1000\n",
      "            prob_curtag_given_prevtwotags = 0\n",
      "            prob_curtag_given_prevword = word_arr[word_index_mapper[prev_word],pos_tags_index_mapper[cur_tag]]*1000\n",
      "        else:\n",
      "            (prev_prev_word, prev_prev_tag) = line[i-2]\n",
      "            (prev_word, prev_tag) = line[i-1]\n",
      "            (cur_word, cur_tag) = line[i]\n",
      "            prob_prevtag_given_curtag = bi_arr[pos_tags_index_mapper[cur_tag],pos_tags_index_mapper[prev_tag]]*1000\n",
      "            prob_curtag_given_prevtag = bi_arr[pos_tags_index_mapper[prev_tag],pos_tags_index_mapper[cur_tag]]*1000\n",
      "            prob_curtag_given_prevtwotags = tri_arr[pos_tags_index_mapper[prev_prev_tag],pos_tags_index_mapper[prev_tag],pos_tags_index_mapper[cur_tag]]*1000\n",
      "            prob_curtag_given_prevword = word_arr[word_index_mapper[prev_word],pos_tags_index_mapper[cur_tag]]*1000\n",
      "        #print prob_curtag_given_prevtag, prob_curtag_given_prevtwotags, prob_curtag_given_prevword\n",
      "        total_prob = prob_prevtag_given_curtag + prob_curtag_given_prevtag + prob_curtag_given_prevtwotags + 10*prob_curtag_given_prevword\n",
      "        if (total_prob < local_min):\n",
      "            local_min = total_prob\n",
      "            local_pos = i\n",
      "    \n",
      "    # Get the missing tag now that the location is known\n",
      "    \n",
      "    #print new_line\n",
      "    max_prob = 0\n",
      "    max_prob_tag = \"\"\n",
      "    for tag in pos_tags_index_mapper:\n",
      "        (prev_word, prev_tag) = line[local_pos-1]\n",
      "        prob_tag_given_prevtag = bi_arr[pos_tags_index_mapper[prev_tag],pos_tags_index_mapper[tag]]*1000\n",
      "        prob_tag_given_prevword = word_arr[word_index_mapper[prev_word],pos_tags_index_mapper[tag]]*1000\n",
      "        total_prob = prob_tag_given_prevtag + prob_tag_given_prevword\n",
      "        if (total_prob > max_prob):\n",
      "            max_prob = total_prob\n",
      "            max_prob_tag = tag\n",
      "    new_line = line[:local_pos]\n",
      "    new_line.append(('missing', max_prob_tag))\n",
      "    new_line.append(line[local_pos:])\n",
      "    output.append(new_line)\n",
      "    #print new_line\n",
      "    #print\n",
      "\n",
      "save_to_disk(output, output_path)    \n",
      "#print(bi_arr[pos_tags_index_mapper['VB'],pos_tags_index_mapper['MD']]*1000)\n",
      "#print(bi_arr[pos_tags_index_mapper['MD'],pos_tags_index_mapper['VB']]*1000)\n",
      "#print(tri_arr[pos_tags_index_mapper['NN'],pos_tags_index_mapper['MD'],pos_tags_index_mapper['VB']]*1000)\n",
      "#print(word_arr[word_index_mapper['may'],pos_tags_index_mapper['VB']]*1000)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Unpickling...\n",
        "Done Pickling...\n",
        "Unpickling...\n",
        "Done Pickling...\n",
        "Unpickling...\n",
        "Done Pickling..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "('Pickling to... ', '/home/stufs1/nitigupta/628/pickled_ds/sents_with_predict_tags.txt')\n",
        "Done Pickling...\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tagged_sents = load_from_disk(output_path)\n",
      "pos_prob = load_from_disk(pos_prob_path)\n",
      "tri_arr = load_from_disk(tri_arr_path)\n",
      "word_arr = load_from_disk(word_pos_path)\n",
      "pos_word_dict = load_from_disk(pos_word_dict_path)\n",
      "bi_word = load_from_disk(bi_word_path)\n",
      "word_curpos = load_from_disk(word_curpos_path)\n",
      "output_sents = []\n",
      "#print pos_word_dict\n",
      "#print bi_word['back']\n",
      "\n",
      "for line in tagged_sents:\n",
      "    local_max = 0\n",
      "    missing_word = \"\"\n",
      "    missing_tag = \"\"\n",
      "    local_pos = 0\n",
      "    new_line = []\n",
      "    for i in range(1,len(line)):\n",
      "        (cur_word, cur_tag) = line[i]\n",
      "        if (cur_word == \"missing\"):\n",
      "            word_list = pos_word_dict[cur_tag]\n",
      "            #print cur_tag, \"===\", word_list\n",
      "            for word in word_list.keys():\n",
      "                #print bi_word[prev_word]\n",
      "                if (word_index_mapper.has_key(word) == False):\n",
      "                    continue\n",
      "                if (i == 1):\n",
      "                    (prev_word, prev_tag) = line[i-1]\n",
      "                    prob_curtag = pos_prob[pos_tags_index_mapper[cur_tag]]*1000\n",
      "                    prob_curtag_given_curword = word_curpos[word_index_mapper[word],pos_tags_index_mapper[cur_tag]]*1000\n",
      "                    prob_curtag_given_prevtwotags = 1\n",
      "                    prob_curword_given_prevword = 0.001\n",
      "                    if (bi_word.has_key(prev_word)):\n",
      "                        dict_list = bi_word[prev_word]\n",
      "                        if (dict_list.has_key(word)):\n",
      "                            prob_curword_given_prevword = bi_word[prev_word][word]*1000                                               \n",
      "                    final_prob = prob_curword_given_prevword*((prob_curtag_given_curword*prob_curtag_given_prevtwotags)/prob_curtag)\n",
      "                else:\n",
      "                    (prev_prev_word, prev_prev_tag) = line[i-2]\n",
      "                    (prev_word, prev_tag) = line[i-1]\n",
      "                    prob_curtag = pos_prob[pos_tags_index_mapper[cur_tag]]*1000\n",
      "                    prob_curtag_given_curword = word_curpos[word_index_mapper[word],pos_tags_index_mapper[cur_tag]]*1000\n",
      "                    prob_curtag_given_prevtwotags = tri_arr[pos_tags_index_mapper[prev_prev_tag],pos_tags_index_mapper[prev_tag],pos_tags_index_mapper[cur_tag]]*1000\n",
      "                    prob_curword_given_prevword = 0.001\n",
      "                    if (bi_word.has_key(prev_word)):\n",
      "                        dict_list = bi_word[prev_word]\n",
      "                        if (dict_list.has_key(word)):\n",
      "                            prob_curword_given_prevword = bi_word[prev_word][word]*1000                                               \n",
      "                    final_prob = prob_curword_given_prevword*((prob_curtag_given_curword*prob_curtag_given_prevtwotags)/prob_curtag)\n",
      "                if (final_prob > local_max):\n",
      "                    local_max = final_prob\n",
      "                    missing_word = word\n",
      "                    local_pos = i\n",
      "                    missing_tag = cur_tag\n",
      "            break\n",
      "        \n",
      "    new_line = line[:local_pos]\n",
      "    new_line.append((missing_word, missing_tag))\n",
      "    new_line.append(line[local_pos+1:])\n",
      "    new_line.append((local_max, missing_word))\n",
      "    output_sents.append(new_line)\n",
      "    \n",
      "    \n",
      "for line in output_sents:\n",
      "    print\n",
      "    print line"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Unpickling...\n",
        "Done Pickling...\n",
        "Unpickling...\n",
        "Done Pickling...\n",
        "Unpickling...\n",
        "Done Pickling...\n",
        "Unpickling...\n",
        "Done Pickling..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Unpickling...\n",
        "Done Pickling..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Unpickling...\n",
        "Done Pickling..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Unpickling...\n",
        "Done Pickling..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "\n",
        "[('The', 'DT'), ('other', 'JJ'), ('two', 'CD'), ('were', 'VBD'), ('killed', 'VBN'), ('up', 'RP'), [[('action', 'NN'), ('on', 'IN'), ('Thursday', 'NNP')]], (4.0472982256485679, 'up')]\n",
        "\n",
        "[('She', 'PRP'), ('was', 'VBD'), ('reported', 'VBN'), ('about', 'RP'), [[('missing', 'VBG'), ('on', 'IN'), ('the', 'DT'), ('of', 'IN'), ('Friday', 'NNP'), ('18', 'CD'), ('December', 'NNP')]], (116.56964935623755, 'about')]\n",
        "\n",
        "[('They', 'PRP'), ('are', 'VBP'), ('as', 'RB'), ('earlier', 'RBR'), [[('strong', 'JJ'), ('lions', 'NNS')]], (1207.6966868733671, 'earlier')]\n",
        "\n",
        "[('The', 'DT'), ('hotel', 'NN'), ('industry', 'NN'), ('may', 'MD'), ('have', 'VB'), [[('suffering', 'VBG'), (',', ','), ('but', 'CC'), ('you', 'PRP'), ('wouldn', 'VBP'), (\"'\", \"''\"), ('t', 'NN'), ('know', 'VBP'), ('it', 'PRP'), ('by', 'IN'), ('looking', 'VBG'), ('at', 'IN'), ('the', 'DT'), ('lineup', 'NN'), ('of', 'IN'), ('new', 'JJ'), ('Westin', 'NNP'), ('Hotels', 'NNPS')]], (542.48005440994677, 'have')]\n"
       ]
      }
     ],
     "prompt_number": 46
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}